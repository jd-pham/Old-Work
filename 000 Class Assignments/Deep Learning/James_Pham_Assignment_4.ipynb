{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d9bd60ff8a7a5aba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#import libraries and functions to load the data\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "# from torchvision.datasets import FashionMNIST as mnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Device (cpu, cuda, mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is cuda\n"
     ]
    }
   ],
   "source": [
    "#for nvidia \n",
    "has_gpu = torch.cuda.is_available()\n",
    "#for mac\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")\n",
    "\n",
    "## Some Hyperparameters \n",
    "random_seed = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "num_epochs = 15\n",
    "num_classes = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Transforms for Preprocessing Data from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(32),\n",
    "        transforms.RandomCrop((28,28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "    ]\n",
    ")\n",
    "test_data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(32),\n",
    "        transforms.CenterCrop((28,28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloand the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = ImageFolder(root=\"mnist-imgs/train\", transform=train_data_transform)\n",
    "#test_dataset = ImageFolder(root=\"mnist-imgs/test\", transform=test_data_transform)\n",
    "\n",
    "train_dataset = datasets.FashionMNIST('./FashionMNIST/', train=True, transform=train_data_transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST('./FashionMNIST/', train=False, transform=test_data_transform, download=True)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(train_dataset, lengths=[55000, 5000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders to get mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    num_workers=3,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    num_workers=3,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    num_workers=3,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.cnn_features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            \n",
    "            torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            \n",
    "            \n",
    "            # torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            # torch.nn.BatchNorm2d(16)\n",
    "            # torch.nn.ReLU(inplace=True),\n",
    "            # torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            # torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            # torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.AvgPool2d(kernel_size=3, stride=2, padding=0),\n",
    "        )\n",
    "        self.fc_features = torch.nn.Sequential(\n",
    "            # torch.nn.AdaptiveAvgPool2d(1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(2704, 100),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(100, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_features(x)\n",
    "        x = self.fc_features(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        model = model.train()\n",
    "        true_pred = 0\n",
    "        tot_samples = 0\n",
    "        for imgs, labels in data_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            _, label_pred = torch.max(logits, axis=1)\n",
    "            true_pred += (label_pred==labels).sum()\n",
    "            tot_samples += labels.shape[0]\n",
    "        acc = (true_pred/float(tot_samples))*100\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch: 100/429 | Loss: 0.5877\n",
      "Epoch: 001/015 | Batch: 200/429 | Loss: 0.5758\n",
      "Epoch: 001/015 | Batch: 300/429 | Loss: 0.4897\n",
      "Epoch: 001/015 | Batch: 400/429 | Loss: 0.4552\n",
      "Epoch: 001 Learning Rate 0.01000000 -> 0.01000000\n",
      "Train Accuracy: 82.026\n",
      "Validation Accuracy: 81.760\n",
      "Time elapsed so far: 1.69 min\n",
      "Epoch: 002/015 | Batch: 100/429 | Loss: 0.3974\n",
      "Epoch: 002/015 | Batch: 200/429 | Loss: 0.3647\n",
      "Epoch: 002/015 | Batch: 300/429 | Loss: 0.4821\n",
      "Epoch: 002/015 | Batch: 400/429 | Loss: 0.2986\n",
      "Epoch: 002 Learning Rate 0.01000000 -> 0.00500000\n",
      "Train Accuracy: 85.539\n",
      "Validation Accuracy: 85.600\n",
      "Time elapsed so far: 3.49 min\n",
      "Epoch: 003/015 | Batch: 100/429 | Loss: 0.2461\n",
      "Epoch: 003/015 | Batch: 200/429 | Loss: 0.2427\n",
      "Epoch: 003/015 | Batch: 300/429 | Loss: 0.2657\n",
      "Epoch: 003/015 | Batch: 400/429 | Loss: 0.2603\n",
      "Epoch: 003 Learning Rate 0.00500000 -> 0.00500000\n",
      "Train Accuracy: 87.895\n",
      "Validation Accuracy: 87.820\n",
      "Time elapsed so far: 5.32 min\n",
      "Epoch: 004/015 | Batch: 100/429 | Loss: 0.4248\n",
      "Epoch: 004/015 | Batch: 200/429 | Loss: 0.3684\n",
      "Epoch: 004/015 | Batch: 300/429 | Loss: 0.5143\n",
      "Epoch: 004/015 | Batch: 400/429 | Loss: 0.4721\n",
      "Epoch: 004 Learning Rate 0.00500000 -> 0.00250000\n",
      "Train Accuracy: 87.864\n",
      "Validation Accuracy: 87.640\n",
      "Time elapsed so far: 6.73 min\n",
      "Epoch: 005/015 | Batch: 100/429 | Loss: 0.2384\n",
      "Epoch: 005/015 | Batch: 200/429 | Loss: 0.2696\n",
      "Epoch: 005/015 | Batch: 300/429 | Loss: 0.2901\n",
      "Epoch: 005/015 | Batch: 400/429 | Loss: 0.3144\n",
      "Epoch: 005 Learning Rate 0.00250000 -> 0.00250000\n",
      "Train Accuracy: 89.704\n",
      "Validation Accuracy: 89.000\n",
      "Time elapsed so far: 7.90 min\n",
      "Epoch: 006/015 | Batch: 100/429 | Loss: 0.2227\n",
      "Epoch: 006/015 | Batch: 200/429 | Loss: 0.2767\n",
      "Epoch: 006/015 | Batch: 300/429 | Loss: 0.3857\n",
      "Epoch: 006/015 | Batch: 400/429 | Loss: 0.2548\n",
      "Epoch: 006 Learning Rate 0.00250000 -> 0.00125000\n",
      "Train Accuracy: 89.789\n",
      "Validation Accuracy: 89.140\n",
      "Time elapsed so far: 9.03 min\n",
      "Epoch: 007/015 | Batch: 100/429 | Loss: 0.2395\n",
      "Epoch: 007/015 | Batch: 200/429 | Loss: 0.2480\n",
      "Epoch: 007/015 | Batch: 300/429 | Loss: 0.2303\n",
      "Epoch: 007/015 | Batch: 400/429 | Loss: 0.1910\n",
      "Epoch: 007 Learning Rate 0.00125000 -> 0.00125000\n",
      "Train Accuracy: 90.720\n",
      "Validation Accuracy: 90.420\n",
      "Time elapsed so far: 10.25 min\n",
      "Epoch: 008/015 | Batch: 100/429 | Loss: 0.2990\n",
      "Epoch: 008/015 | Batch: 200/429 | Loss: 0.2801\n",
      "Epoch: 008/015 | Batch: 300/429 | Loss: 0.1854\n",
      "Epoch: 008/015 | Batch: 400/429 | Loss: 0.2607\n",
      "Epoch: 008 Learning Rate 0.00125000 -> 0.00062500\n",
      "Train Accuracy: 91.343\n",
      "Validation Accuracy: 89.640\n",
      "Time elapsed so far: 11.35 min\n",
      "Epoch: 009/015 | Batch: 100/429 | Loss: 0.2448\n",
      "Epoch: 009/015 | Batch: 200/429 | Loss: 0.2360\n",
      "Epoch: 009/015 | Batch: 300/429 | Loss: 0.2889\n",
      "Epoch: 009/015 | Batch: 400/429 | Loss: 0.2700\n",
      "Epoch: 009 Learning Rate 0.00062500 -> 0.00062500\n",
      "Train Accuracy: 91.508\n",
      "Validation Accuracy: 90.060\n",
      "Time elapsed so far: 12.44 min\n",
      "Epoch: 010/015 | Batch: 100/429 | Loss: 0.3207\n",
      "Epoch: 010/015 | Batch: 200/429 | Loss: 0.2328\n",
      "Epoch: 010/015 | Batch: 300/429 | Loss: 0.2426\n",
      "Epoch: 010/015 | Batch: 400/429 | Loss: 0.2000\n",
      "Epoch: 010 Learning Rate 0.00062500 -> 0.00031250\n",
      "Train Accuracy: 91.749\n",
      "Validation Accuracy: 90.580\n",
      "Time elapsed so far: 13.61 min\n",
      "Epoch: 011/015 | Batch: 100/429 | Loss: 0.1585\n",
      "Epoch: 011/015 | Batch: 200/429 | Loss: 0.2789\n",
      "Epoch: 011/015 | Batch: 300/429 | Loss: 0.1759\n",
      "Epoch: 011/015 | Batch: 400/429 | Loss: 0.2116\n",
      "Epoch: 011 Learning Rate 0.00031250 -> 0.00031250\n",
      "Train Accuracy: 91.854\n",
      "Validation Accuracy: 90.900\n",
      "Time elapsed so far: 14.81 min\n",
      "Epoch: 012/015 | Batch: 100/429 | Loss: 0.2275\n",
      "Epoch: 012/015 | Batch: 200/429 | Loss: 0.2626\n",
      "Epoch: 012/015 | Batch: 300/429 | Loss: 0.2226\n",
      "Epoch: 012/015 | Batch: 400/429 | Loss: 0.1722\n",
      "Epoch: 012 Learning Rate 0.00031250 -> 0.00015625\n",
      "Train Accuracy: 92.080\n",
      "Validation Accuracy: 90.440\n",
      "Time elapsed so far: 16.03 min\n",
      "Epoch: 013/015 | Batch: 100/429 | Loss: 0.2718\n",
      "Epoch: 013/015 | Batch: 200/429 | Loss: 0.2073\n",
      "Epoch: 013/015 | Batch: 300/429 | Loss: 0.1326\n",
      "Epoch: 013/015 | Batch: 400/429 | Loss: 0.1914\n",
      "Epoch: 013 Learning Rate 0.00015625 -> 0.00015625\n",
      "Train Accuracy: 92.182\n",
      "Validation Accuracy: 90.580\n",
      "Time elapsed so far: 17.23 min\n",
      "Epoch: 014/015 | Batch: 100/429 | Loss: 0.1991\n",
      "Epoch: 014/015 | Batch: 200/429 | Loss: 0.1883\n",
      "Epoch: 014/015 | Batch: 300/429 | Loss: 0.1818\n",
      "Epoch: 014/015 | Batch: 400/429 | Loss: 0.2317\n",
      "Epoch: 014 Learning Rate 0.00015625 -> 0.00007813\n",
      "Train Accuracy: 92.391\n",
      "Validation Accuracy: 90.720\n",
      "Time elapsed so far: 18.43 min\n",
      "Epoch: 015/015 | Batch: 100/429 | Loss: 0.1501\n",
      "Epoch: 015/015 | Batch: 200/429 | Loss: 0.1706\n",
      "Epoch: 015/015 | Batch: 300/429 | Loss: 0.2239\n",
      "Epoch: 015/015 | Batch: 400/429 | Loss: 0.1627\n",
      "Epoch: 015 Learning Rate 0.00007813 -> 0.00007813\n",
      "Train Accuracy: 92.350\n",
      "Validation Accuracy: 91.320\n",
      "Time elapsed so far: 19.65 min\n",
      "Total Train Time: 19.65 min\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model = CNN(num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "epoch_loss = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        ## Forward Propagation - extract features and classify\n",
    "        logits = model(imgs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        loss_list.append(float(loss))\n",
    "\n",
    "        #zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        #estimate new gradients\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1:03d}/{num_epochs:03d} | \"\n",
    "                f\"Batch: {batch_idx + 1:03d}/{len(train_loader):03d} | \"\n",
    "                f\"Loss: {loss:.4f}\"\n",
    "            )\n",
    "    mean_loss = round(np.mean(loss_list),5)\n",
    "    epoch_loss.append(mean_loss)\n",
    "    # Tracking the Learning Rate Scheduler\n",
    "    prev_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"Epoch: {epoch+1:03d} Learning Rate {prev_lr:.8f} -> {current_lr:.8f}\")\n",
    "\n",
    "\n",
    "    # Evaluate Performance after each epoch\n",
    "    model = model.eval()\n",
    "    tr_acc = accuracy(model, train_loader, device)\n",
    "    valid_acc = accuracy(model, valid_loader, device)\n",
    "    print(f\"Train Accuracy: {tr_acc:0.3f}\")\n",
    "    print(f\"Validation Accuracy: {valid_acc:0.3f}\")\n",
    "    print(f\"Time elapsed so far: {(time.time() - start) / 60:.2f} min\")\n",
    "\n",
    "print(f\"Total Train Time: {(time.time() - start) / 60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59508, 0.42037, 0.3396, 0.32214, 0.28674, 0.27491, 0.25423, 0.24645, 0.23286, 0.22714, 0.2182, 0.21622, 0.21432, 0.20892, 0.20706]\n"
     ]
    }
   ],
   "source": [
    "print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b333090280>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = [i for i in range(1,num_epochs + 1)]\n",
    "plt.plot(epochs, epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.350\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "ts_acc = accuracy(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {ts_acc:0.3f}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2796f898c241be261d17d4440712f4c58a6e72713839fe7ec4d10cc08aa00573"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
